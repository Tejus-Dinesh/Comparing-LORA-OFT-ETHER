{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "9HG4EFylutGy"
      },
      "outputs": [],
      "source": [
        "!pip install rouge_score datasets transformers wandb nltk psutil  -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoIKxk6wu4DX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
        "from transformers import Trainer, TrainerCallback\n",
        "from datasets import load_dataset, Dataset\n",
        "import os\n",
        "import gc\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import json\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "import logging\n",
        "import psutil\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwTKXynNu82K",
        "outputId": "3ba6fb13-f9a0-4cf9-d3dd-947af222f31b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(\"ether_training.log\"),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "E_SwVpyrhqzR"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Memory utilities\n",
        "def clear_memory():\n",
        "    \"\"\"Clear unused memory.\"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "def get_memory_stats():\n",
        "    \"\"\"Get detailed memory usage statistics.\"\"\"\n",
        "    stats = {}\n",
        "\n",
        "    # GPU memory\n",
        "    if torch.cuda.is_available():\n",
        "        stats[\"gpu_allocated_gb\"] = torch.cuda.memory_allocated() / (1024**3)\n",
        "        stats[\"gpu_reserved_gb\"] = torch.cuda.memory_reserved() / (1024**3)\n",
        "        stats[\"gpu_max_allocated_gb\"] = torch.cuda.max_memory_allocated() / (1024**3)\n",
        "        stats[\"gpu_max_reserved_gb\"] = torch.cuda.max_memory_reserved() / (1024**3)\n",
        "\n",
        "        # Reset max memory stats for future tracking\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    # CPU memory\n",
        "    mem = psutil.virtual_memory()\n",
        "    stats[\"cpu_used_gb\"] = (mem.total - mem.available) / (1024**3)\n",
        "    stats[\"cpu_total_gb\"] = mem.total / (1024**3)\n",
        "    stats[\"cpu_percent\"] = mem.percent\n",
        "\n",
        "    return stats\n",
        "\n",
        "def print_gpu_memory():\n",
        "    \"\"\"Print GPU memory usage.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "        print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0twBMX_HurNo"
      },
      "outputs": [],
      "source": [
        "class COMPUTEMETRICS():\n",
        "    \"\"\"Class to calculate ROUGE and BLEU scores.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "        self.smooth = SmoothingFunction().method1\n",
        "\n",
        "    def calculate_rouge(self, prediction, reference):\n",
        "        \"\"\"Calculate ROUGE scores for a prediction against a reference.\"\"\"\n",
        "        # Handle empty predictions/references\n",
        "        if not prediction or not reference:\n",
        "            return {\n",
        "                \"rouge1\": 0.0,\n",
        "                \"rouge2\": 0.0,\n",
        "                \"rougeL\": 0.0\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            scores = self.rouge_scorer.score(prediction, reference)\n",
        "            return {\n",
        "                \"rouge1\": scores[\"rouge1\"].fmeasure,\n",
        "                \"rouge2\": scores[\"rouge2\"].fmeasure,\n",
        "                \"rougeL\": scores[\"rougeL\"].fmeasure\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error calculating ROUGE score: {e}\")\n",
        "            return {\n",
        "                \"rouge1\": 0.0,\n",
        "                \"rouge2\": 0.0,\n",
        "                \"rougeL\": 0.0\n",
        "            }\n",
        "\n",
        "    def calculate_bleu(self, prediction, reference):\n",
        "        \"\"\"Calculate BLEU score for a prediction against a reference.\"\"\"\n",
        "        # Handle empty predictions/references\n",
        "        if not prediction or not reference:\n",
        "            return 0.0\n",
        "\n",
        "        try:\n",
        "            prediction_tokens = nltk.word_tokenize(prediction.lower())\n",
        "            reference_tokens = [nltk.word_tokenize(reference.lower())]\n",
        "\n",
        "            # Handle empty token sequences\n",
        "            if not prediction_tokens or not reference_tokens[0]:\n",
        "                return 0.0\n",
        "\n",
        "            return sentence_bleu(reference_tokens, prediction_tokens, smoothing_function=self.smooth)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error calculating BLEU score: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    def calculate_metrics(self, predictions, references):\n",
        "        \"\"\"Calculate all metrics for a batch of predictions and references.\"\"\"\n",
        "        if not predictions or not references:\n",
        "            return {\n",
        "                \"rouge1\": 0.0,\n",
        "                \"rouge2\": 0.0,\n",
        "                \"rougeL\": 0.0,\n",
        "                \"bleu\": 0.0\n",
        "            }\n",
        "\n",
        "        metrics = {\n",
        "            \"rouge1\": [],\n",
        "            \"rouge2\": [],\n",
        "            \"rougeL\": [],\n",
        "            \"bleu\": []\n",
        "        }\n",
        "\n",
        "        for pred, ref in zip(predictions, references):\n",
        "            # Calculate ROUGE\n",
        "            rouge_scores = self.calculate_rouge(pred, ref)\n",
        "            for key in [\"rouge1\", \"rouge2\", \"rougeL\"]:\n",
        "                metrics[key].append(rouge_scores[key])\n",
        "\n",
        "            # Calculate BLEU\n",
        "            metrics[\"bleu\"].append(self.calculate_bleu(pred, ref))\n",
        "\n",
        "        # Average the metrics and handle empty lists\n",
        "        return {key: sum(values) / len(values) if values else 0 for key, values in metrics.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xkqZBCWtvHDw"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# ETHER Layer Implementations\n",
        "class HouseholderTransform(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of the Householder transformation for ETHER.\n",
        "    H = I - 2uu^T where u is a unit vector.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, n_blocks=1):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.n_blocks = n_blocks\n",
        "\n",
        "        # If using blocks, divide dimension\n",
        "        self.block_size = dim // n_blocks\n",
        "        if self.block_size == 0:\n",
        "            self.block_size = dim\n",
        "            self.n_blocks = 1\n",
        "\n",
        "        # Initialize unit normal vectors for each block\n",
        "        if self.n_blocks > 1:\n",
        "            self.u_vectors = nn.ParameterList([\n",
        "                nn.Parameter(torch.zeros(self.block_size))\n",
        "                for _ in range(self.n_blocks)\n",
        "            ])\n",
        "            # Initialize with small values\n",
        "            for u in self.u_vectors:\n",
        "                nn.init.normal_(u, mean=0, std=0.01)\n",
        "        else:\n",
        "            # Single vector for the entire dimension\n",
        "            self.u = nn.Parameter(torch.zeros(dim))\n",
        "            nn.init.normal_(self.u, mean=0, std=0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Apply Householder transformation: H = I - 2uu^T\n",
        "\n",
        "        \"\"\"\n",
        "        # Process input in blocks if using multiple blocks\n",
        "        if self.n_blocks > 1:\n",
        "            results = []\n",
        "\n",
        "            for i, u_param in enumerate(self.u_vectors):\n",
        "                # Get the block from input\n",
        "                start_idx = i * self.block_size\n",
        "                end_idx = min(start_idx + self.block_size, self.dim)\n",
        "                if start_idx >= x.shape[-1]:\n",
        "                    continue\n",
        "                x_block = x[..., start_idx:end_idx]\n",
        "\n",
        "                # Normalize u to ensure it's a unit vector\n",
        "                u = F.normalize(u_param, p=2, dim=0)\n",
        "\n",
        "                # Compute the Householder transformation: x - 2u(u^T x)\n",
        "                u_dot_x = torch.matmul(x_block, u)\n",
        "                reflection = 2 * u_dot_x.unsqueeze(-1) * u.unsqueeze(0)\n",
        "                transformed_block = x_block - reflection\n",
        "\n",
        "                results.append(transformed_block)\n",
        "\n",
        "            # Concatenate transformed blocks\n",
        "            return torch.cat(results, dim=-1)\n",
        "        else:\n",
        "            # Normalize u to ensure it's a unit vector\n",
        "            u = F.normalize(self.u, p=2, dim=0)\n",
        "\n",
        "            # Compute the Householder transformation: x - 2u(u^T x)\n",
        "            u_dot_x = torch.matmul(x, u)\n",
        "            reflection = 2 * u_dot_x.unsqueeze(-1) * u.unsqueeze(0)\n",
        "            return x - reflection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JfdNkcAdvU54"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ETHERPlusTransform(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of ETHER+, a relaxation of ETHER.\n",
        "    H+ = I - uu^T + vv^T\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, n_blocks=1):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.n_blocks = n_blocks\n",
        "\n",
        "        # If using blocks, divide dimension\n",
        "        self.block_size = dim // n_blocks\n",
        "        if self.block_size == 0:\n",
        "            self.block_size = dim\n",
        "            self.n_blocks = 1\n",
        "\n",
        "        # Initialize two sets of vectors for each block\n",
        "        if self.n_blocks > 1:\n",
        "            self.u_vectors = nn.ParameterList([\n",
        "                nn.Parameter(torch.zeros(self.block_size))\n",
        "                for _ in range(self.n_blocks)\n",
        "            ])\n",
        "            self.v_vectors = nn.ParameterList([\n",
        "                nn.Parameter(torch.zeros(self.block_size))\n",
        "                for _ in range(self.n_blocks)\n",
        "            ])\n",
        "            # Initialize with small values\n",
        "            for u, v in zip(self.u_vectors, self.v_vectors):\n",
        "                nn.init.normal_(u, mean=0, std=0.01)\n",
        "                nn.init.normal_(v, mean=0, std=0.01)\n",
        "        else:\n",
        "            # Single vectors for the entire dimension\n",
        "            self.u = nn.Parameter(torch.zeros(dim))\n",
        "            self.v = nn.Parameter(torch.zeros(dim))\n",
        "            nn.init.normal_(self.u, mean=0, std=0.01)\n",
        "            nn.init.normal_(self.v, mean=0, std=0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Apply ETHER+ transformation: H+ = I - uu^T + vv^T\n",
        "\n",
        "        \"\"\"\n",
        "        # Process input in blocks if using multiple blocks\n",
        "        if self.n_blocks > 1:\n",
        "            results = []\n",
        "\n",
        "            for i, (u_param, v_param) in enumerate(zip(self.u_vectors, self.v_vectors)):\n",
        "                # Get the block from input\n",
        "                start_idx = i * self.block_size\n",
        "                end_idx = min(start_idx + self.block_size, self.dim)\n",
        "                if start_idx >= x.shape[-1]:\n",
        "                    continue\n",
        "                x_block = x[..., start_idx:end_idx]\n",
        "\n",
        "                # Normalize vectors to ensure they're unit vectors\n",
        "                u = F.normalize(u_param, p=2, dim=0)\n",
        "                v = F.normalize(v_param, p=2, dim=0)\n",
        "\n",
        "                # Compute the relaxed transformation: x - u(u^T x) + v(v^T x)\n",
        "                u_dot_x = torch.matmul(x_block, u)\n",
        "                v_dot_x = torch.matmul(x_block, v)\n",
        "\n",
        "                u_term = u_dot_x.unsqueeze(-1) * u.unsqueeze(0)\n",
        "                v_term = v_dot_x.unsqueeze(-1) * v.unsqueeze(0)\n",
        "\n",
        "                transformed_block = x_block - u_term + v_term\n",
        "\n",
        "                results.append(transformed_block)\n",
        "\n",
        "            # Concatenate transformed blocks\n",
        "            return torch.cat(results, dim=-1)\n",
        "        else:\n",
        "            # Normalize vectors to ensure they're unit vectors\n",
        "            u = F.normalize(self.u, p=2, dim=0)\n",
        "            v = F.normalize(self.v, p=2, dim=0)\n",
        "\n",
        "            # Compute the relaxed transformation: x - u(u^T x) + v(v^T x)\n",
        "            u_dot_x = torch.matmul(x, u)\n",
        "            v_dot_x = torch.matmul(x, v)\n",
        "\n",
        "            u_term = u_dot_x.unsqueeze(-1) * u.unsqueeze(0)\n",
        "            v_term = v_dot_x.unsqueeze(-1) * v.unsqueeze(0)\n",
        "\n",
        "            return x - u_term + v_term\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eUNMxZ3ZvaQz"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ETHERLinear(nn.Module):\n",
        "    \"\"\"\n",
        "    Linear layer with ETHER transformation.\n",
        "    \"\"\"\n",
        "    def __init__(self, base_layer, use_ether_plus=True, n_blocks=16, double_sided=True):\n",
        "        super().__init__()\n",
        "        self.in_features = base_layer.in_features\n",
        "        self.out_features = base_layer.out_features\n",
        "        self.use_ether_plus = use_ether_plus\n",
        "        self.double_sided = double_sided\n",
        "\n",
        "        # Store original weights and bias, but freeze them\n",
        "        self.weight = nn.Parameter(base_layer.weight.data.clone(), requires_grad=False)\n",
        "        self.bias = None\n",
        "        if hasattr(base_layer, 'bias') and base_layer.bias is not None:\n",
        "            self.bias = nn.Parameter(base_layer.bias.data.clone(), requires_grad=False)\n",
        "\n",
        "        # Create the transformation modules\n",
        "        if use_ether_plus:\n",
        "            # For ETHER+, we use ether_plus on both sides for better performance\n",
        "            self.left_transform = ETHERPlusTransform(self.in_features, n_blocks)\n",
        "            if double_sided:\n",
        "                self.right_transform = ETHERPlusTransform(self.out_features, n_blocks)\n",
        "        else:\n",
        "            # For basic ETHER, we use Householder on the left\n",
        "            self.left_transform = HouseholderTransform(self.in_features, n_blocks)\n",
        "            if double_sided:\n",
        "                self.right_transform = HouseholderTransform(self.out_features, n_blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass with ETHER transformation.\n",
        "        \"\"\"\n",
        "        # Make sure the input is on the same device as the weights\n",
        "        device = self.weight.device\n",
        "        x = x.to(device)\n",
        "\n",
        "        # Apply left transform, and compute W Â· x\n",
        "        x_transformed = self.left_transform(x)\n",
        "        output = F.linear(x_transformed, self.weight, bias=None)\n",
        "\n",
        "        # Apply right transform if double-sided\n",
        "        if self.double_sided:\n",
        "            output = self.right_transform(output)\n",
        "\n",
        "        # Add bias if present\n",
        "        if self.bias is not None:\n",
        "            output = output + self.bias\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Rjp0fgV5vdwY"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to apply ETHER to a model\n",
        "def apply_ether_to_model(model, use_ether_plus=True, target_modules=None, n_blocks=16, double_sided=True):\n",
        "    \"\"\"\n",
        "    Apply ETHER transformations to target modules in the model.\n",
        "\n",
        "    \"\"\"\n",
        "    if target_modules is None:\n",
        "        # Default set to attention layers\n",
        "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        "\n",
        "    # Count transformations for reporting\n",
        "    transformation_count = 0\n",
        "    total_trainable_params = 0\n",
        "\n",
        "    # Inspect model architecture\n",
        "    logger.info(\"Model architecture inspection:\")\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            logger.info(f\"Linear layer found: {name}, in_features={module.in_features}, out_features={module.out_features}\")\n",
        "\n",
        "\n",
        "    # Helper function to recursively replace modules\n",
        "    def replace_module(module, path=\"\"):\n",
        "        nonlocal transformation_count, total_trainable_params\n",
        "\n",
        "        for name, child in list(module.named_children()):\n",
        "            full_name = f\"{path}.{name}\" if path else name\n",
        "\n",
        "            # Checking for target module\n",
        "            if isinstance(child, nn.Linear) and any(target in full_name for target in target_modules):\n",
        "                # Replace with ETHER linear layer\n",
        "                ether_layer = ETHERLinear(\n",
        "                    child,\n",
        "                    use_ether_plus=use_ether_plus,\n",
        "                    n_blocks=n_blocks,\n",
        "                    double_sided=double_sided\n",
        "                )\n",
        "\n",
        "                # Count parameters\n",
        "                params = sum(p.numel() for p in ether_layer.parameters() if p.requires_grad)\n",
        "                total_trainable_params += params\n",
        "\n",
        "                # Replace the module\n",
        "                setattr(module, name, ether_layer)\n",
        "                transformation_count += 1\n",
        "                logger.info(f\"Applied ETHER {'Plus' if use_ether_plus else ''} to {full_name} (+{params} params)\")\n",
        "            else:\n",
        "\n",
        "                replace_module(child, full_name)\n",
        "\n",
        "    # Start the replacement process\n",
        "    replace_module(model)\n",
        "\n",
        "    logger.info(f\"Applied ETHER {'Plus' if use_ether_plus else ''} to {transformation_count} modules\")\n",
        "    logger.info(f\"Total trainable parameters: {total_trainable_params:,}\")\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "6LzfAnw7vhGu"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Data preparation\n",
        "def load_alpaca_dataset(tokenizer, max_length=2048, sample_size=None):\n",
        "    \"\"\"\n",
        "    Prepare the Alpaca dataset with proper tokenization.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the dataset\n",
        "        dataset = load_dataset(\"tatsu-lab/alpaca\")\n",
        "        logger.info(f\"Loaded dataset with {len(dataset['train'])} examples\")\n",
        "\n",
        "        if sample_size is not None:\n",
        "\n",
        "            indices = random.sample(range(len(dataset[\"train\"])), min(sample_size, len(dataset[\"train\"])))\n",
        "            dataset[\"train\"] = dataset[\"train\"].select(indices)\n",
        "            logger.info(f\"Sampled {len(dataset['train'])} examples\")\n",
        "\n",
        "        # Tokenize and format the dataset\n",
        "        def preprocess_function(examples):\n",
        "            # Process a batch of examples\n",
        "            batch_size = len(examples[\"instruction\"])\n",
        "            inputs = []\n",
        "            labels = []\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                instruction = examples[\"instruction\"][i]\n",
        "                input_text = examples[\"input\"][i] if examples[\"input\"][i] else \"\"\n",
        "                output = examples[\"output\"][i]\n",
        "\n",
        "                if input_text:\n",
        "                    prompt = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.<|start_header_id|>user<|end_header_id|>\\n{instruction}\\n{input_text}<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
        "                else:\n",
        "                    prompt = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.<|start_header_id|>user<|end_header_id|>\\n{instruction}<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
        "\n",
        "                # Tokenize\n",
        "                tokenized_prompt = tokenizer(prompt, truncation=False, return_tensors=None)\n",
        "                tokenized_output = tokenizer(output + tokenizer.eos_token, truncation=False, return_tensors=None)\n",
        "\n",
        "                # Create labels and input IDs\n",
        "                input_ids = tokenized_prompt[\"input_ids\"]\n",
        "                attention_mask = tokenized_prompt[\"attention_mask\"]\n",
        "                labels_ids = [-100] * len(input_ids)  # Initialize labels with -100\n",
        "\n",
        "                # Add output tokens and corresponding label IDs\n",
        "                input_ids.extend(tokenized_output[\"input_ids\"])\n",
        "                attention_mask.extend(tokenized_output[\"attention_mask\"])\n",
        "                labels_ids.extend(tokenized_output[\"input_ids\"])\n",
        "\n",
        "                # Truncate if too long\n",
        "                max_length = 512  # Define max length\n",
        "                if len(input_ids) > max_length:\n",
        "                    input_ids = input_ids[:max_length]\n",
        "                    attention_mask = attention_mask[:max_length]\n",
        "                    labels_ids = labels_ids[:max_length]\n",
        "\n",
        "                inputs.append({\n",
        "                    \"input_ids\": input_ids,\n",
        "                    \"attention_mask\": attention_mask\n",
        "                })\n",
        "                labels.append(labels_ids)\n",
        "\n",
        "            batch = {\n",
        "                \"input_ids\": [inp[\"input_ids\"] for inp in inputs],\n",
        "                \"attention_mask\": [inp[\"attention_mask\"] for inp in inputs],\n",
        "                \"labels\": labels\n",
        "            }\n",
        "\n",
        "            return batch\n",
        "\n",
        "        # Process the dataset\n",
        "        processed_dataset = dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            batch_size=100,\n",
        "            remove_columns=dataset[\"train\"].column_names\n",
        "        )\n",
        "\n",
        "        # Split the dataset\n",
        "        train_size = int(0.9 * len(processed_dataset[\"train\"]))\n",
        "        train_dataset = processed_dataset[\"train\"].select(range(train_size))\n",
        "        eval_dataset = processed_dataset[\"train\"].select(range(train_size-50, len(processed_dataset[\"train\"])))\n",
        "\n",
        "        return {\n",
        "            \"train\": train_dataset,\n",
        "            \"eval\": eval_dataset\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error preparing dataset: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "N5SJkbvWvo0j"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Custom collator to handle different sequence lengths\n",
        "class DATACOLLATOR:\n",
        "    \"\"\"\n",
        "    Collator that pads sequences in a batch.\n",
        "    \"\"\"\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    def __call__(self, examples):\n",
        "        # Get max lengths\n",
        "        max_input_length = max(len(example[\"input_ids\"]) for example in examples)\n",
        "        max_label_length = max(len(example[\"labels\"]) for example in examples)\n",
        "        max_attn_length = max(len(example[\"attention_mask\"]) for example in examples)\n",
        "\n",
        "        max_length = max(max_input_length, max_label_length, max_attn_length)\n",
        "\n",
        "        # Initialize batch\n",
        "        batch = {\n",
        "            \"input_ids\": [],\n",
        "            \"attention_mask\": [],\n",
        "            \"labels\": []\n",
        "        }\n",
        "\n",
        "        # Pad each example\n",
        "        for example in examples:\n",
        "            # Pad input_ids\n",
        "            input_ids = example[\"input_ids\"]\n",
        "            padding_length = max_length - len(input_ids)\n",
        "            padded_input_ids = input_ids + [self.pad_token_id] * padding_length\n",
        "            batch[\"input_ids\"].append(padded_input_ids)\n",
        "\n",
        "            # Pad attention mask\n",
        "            attention_mask = example[\"attention_mask\"]\n",
        "            padded_attention_mask = attention_mask + [0] * padding_length\n",
        "            batch[\"attention_mask\"].append(padded_attention_mask)\n",
        "\n",
        "            # Pad labels (-100 is ignored)\n",
        "            labels = example[\"labels\"]\n",
        "            padded_labels = labels + [-100] * padding_length\n",
        "            batch[\"labels\"].append(padded_labels)\n",
        "\n",
        "        # Convert to tensors\n",
        "        batch = {k: torch.tensor(v) for k, v in batch.items()}\n",
        "\n",
        "        return batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "67KjIvNlvzCL"
      },
      "outputs": [],
      "source": [
        "class MetricsCallback(TrainerCallback):\n",
        "    \"\"\"\n",
        "    Track detailed metrics during training.\n",
        "    \"\"\"\n",
        "    def __init__(self, tokenizer, eval_dataset, log_dir=\"./metrics\"):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.eval_dataset = eval_dataset\n",
        "        self.log_dir = log_dir\n",
        "        self.metrics_calculator = COMPUTEMETRICS()\n",
        "\n",
        "        # Create log directory\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "        # Initialize metrics storage\n",
        "        self.gpu_memory_stats = []\n",
        "        self.cpu_memory_stats = []\n",
        "        self.training_time = 0\n",
        "        self.start_time = None\n",
        "        self.step_times = []\n",
        "        self.loss_values = []\n",
        "        self.evaluation_metrics = []\n",
        "        self.best_metrics = {\n",
        "            \"rouge1\": 0,\n",
        "            \"rouge2\": 0,\n",
        "            \"rougeL\": 0,\n",
        "            \"bleu\": 0,\n",
        "            \"step\": 0\n",
        "        }\n",
        "\n",
        "    def on_train_begin(self, args, state, control, **kwargs):\n",
        "        \"\"\"Called at the beginning of training.\"\"\"\n",
        "        self.start_time = time.time()\n",
        "        logger.info(\"Training started\")\n",
        "\n",
        "        # Log initial memory\n",
        "        memory_stats = get_memory_stats()\n",
        "        self.gpu_memory_stats.append({\"step\": 0, **memory_stats})\n",
        "        logger.info(f\"Initial memory: {memory_stats}\")\n",
        "\n",
        "        # Initialize wandb if used\n",
        "        if args.report_to == \"wandb\" and wandb.run is not None:\n",
        "            wandb.log({\"memory\": memory_stats, \"step\": 0})\n",
        "\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        \"\"\"Called at the end of a training step.\"\"\"\n",
        "        # Record time per step\n",
        "        if len(self.step_times) == 0:\n",
        "            self.step_times.append(time.time() - self.start_time)\n",
        "        else:\n",
        "            # Use the time since the last step\n",
        "            self.step_times.append(time.time() - (self.start_time + sum(self.step_times)))\n",
        "\n",
        "        # Log every N steps\n",
        "        if state.global_step % args.logging_steps == 0:\n",
        "            # Get memory stats\n",
        "            memory_stats = get_memory_stats()\n",
        "            self.gpu_memory_stats.append({\"step\": state.global_step, **memory_stats})\n",
        "\n",
        "            # Calculate training speed\n",
        "            recent_step_times = self.step_times[-args.logging_steps:]\n",
        "            avg_step_time = sum(recent_step_times) / len(recent_step_times)\n",
        "            steps_per_second = 1.0 / avg_step_time if avg_step_time > 0 else 0\n",
        "\n",
        "            # Log to console\n",
        "            logger.info(f\"Step {state.global_step}: {avg_step_time:.3f} sec/step, {steps_per_second:.3f} steps/sec\")\n",
        "            logger.info(f\"Memory: {memory_stats['gpu_allocated_gb']:.2f} GB allocated, {memory_stats['gpu_reserved_gb']:.2f} GB reserved\")\n",
        "\n",
        "            # Log to wandb\n",
        "            if args.report_to == \"wandb\" and wandb.run is not None:\n",
        "                wandb.log({\n",
        "                    \"step\": state.global_step,\n",
        "                    \"avg_step_time\": avg_step_time,\n",
        "                    \"steps_per_second\": steps_per_second,\n",
        "                    \"memory\": memory_stats,\n",
        "                })\n",
        "\n",
        "            # Save memory stats to a file\n",
        "            with open(os.path.join(self.log_dir, \"memory_stats.json\"), \"w\") as f:\n",
        "                json.dump(self.gpu_memory_stats, f, indent=2)\n",
        "\n",
        "            # Periodic cleanup\n",
        "            if state.global_step % 50 == 0:\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        \"\"\"Called when logs are available.\"\"\"\n",
        "        if logs is None:\n",
        "            return\n",
        "\n",
        "        # Extract loss\n",
        "        if \"loss\" in logs:\n",
        "            self.loss_values.append({\"step\": state.global_step, \"loss\": logs[\"loss\"]})\n",
        "\n",
        "            # Save loss to file\n",
        "            with open(os.path.join(self.log_dir, \"loss.json\"), \"w\") as f:\n",
        "                json.dump(self.loss_values, f, indent=2)\n",
        "\n",
        "            # Plot loss\n",
        "            if len(self.loss_values) % 10 == 0:\n",
        "                self._plot_loss()\n",
        "\n",
        "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
        "        \"\"\"Called after evaluation.\"\"\"\n",
        "        if metrics is None:\n",
        "            return\n",
        "\n",
        "        # Add step number to metrics\n",
        "        metrics[\"step\"] = state.global_step\n",
        "        self.evaluation_metrics.append(metrics)\n",
        "\n",
        "        # Check if we have a new best model\n",
        "        if metrics.get(\"eval_rouge1\", 0) > self.best_metrics[\"rouge1\"]:\n",
        "            self.best_metrics[\"rouge1\"] = metrics.get(\"eval_rouge1\", 0)\n",
        "            self.best_metrics[\"rouge2\"] = metrics.get(\"eval_rouge2\", 0)\n",
        "            self.best_metrics[\"rougeL\"] = metrics.get(\"eval_rougeL\", 0)\n",
        "            self.best_metrics[\"bleu\"] = metrics.get(\"eval_bleu\", 0)\n",
        "            self.best_metrics[\"step\"] = state.global_step\n",
        "\n",
        "        # Log eval metrics\n",
        "        logger.info(f\"Evaluation at step {state.global_step}:\")\n",
        "        logger.info(f\"  ROUGE-1: {metrics.get('eval_rouge1', 0):.4f}\")\n",
        "        logger.info(f\"  ROUGE-2: {metrics.get('eval_rouge2', 0):.4f}\")\n",
        "        logger.info(f\"  ROUGE-L: {metrics.get('eval_rougeL', 0):.4f}\")\n",
        "        logger.info(f\"  BLEU: {metrics.get('eval_bleu', 0):.4f}\")\n",
        "\n",
        "        # Save eval metrics\n",
        "        with open(os.path.join(self.log_dir, \"eval_metrics.json\"), \"w\") as f:\n",
        "            json.dump(self.evaluation_metrics, f, indent=2)\n",
        "\n",
        "        # Plot metrics\n",
        "        self._plot_metrics()\n",
        "\n",
        "    def on_train_end(self, args, state, control, **kwargs):\n",
        "\n",
        "        # Calculate total training time\n",
        "        self.training_time = time.time() - self.start_time\n",
        "\n",
        "        # Log final stats\n",
        "        logger.info(\"Training completed\")\n",
        "        logger.info(f\"Total training time: {datetime.timedelta(seconds=self.training_time)}\")\n",
        "        logger.info(f\"Average step time: {sum(self.step_times) / len(self.step_times) if self.step_times else 0:.3f} seconds\")\n",
        "        logger.info(f\"Total steps: {state.global_step}\")\n",
        "        logger.info(f\"Best metrics: {self.best_metrics}\")\n",
        "\n",
        "        # Calculate final metrics\n",
        "        final_metrics = self._calculate_final_metrics()\n",
        "\n",
        "        # Save final report\n",
        "        self._save_final_report(state, final_metrics)\n",
        "\n",
        "        # Create and save visualizations\n",
        "        self._plot_loss()\n",
        "        self._plot_metrics()\n",
        "        self._plot_memory_usage()\n",
        "\n",
        "    def _calculate_final_metrics(self):\n",
        "        \"\"\"Calculate comprehensive final metrics on the evaluation set.\"\"\"\n",
        "        return self.best_metrics\n",
        "\n",
        "    def _plot_loss(self):\n",
        "        \"\"\"Plot the loss curve.\"\"\"\n",
        "        if not self.loss_values:\n",
        "            return\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        steps = [item[\"step\"] for item in self.loss_values]\n",
        "        losses = [item[\"loss\"] for item in self.loss_values]\n",
        "        plt.plot(steps, losses)\n",
        "        plt.title(\"Training Loss\")\n",
        "        plt.xlabel(\"Step\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.grid(True)\n",
        "        plt.savefig(os.path.join(self.log_dir, \"loss_curve.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    def _plot_metrics(self):\n",
        "        \"\"\"Plot the evaluation metrics.\"\"\"\n",
        "        if not self.evaluation_metrics:\n",
        "            return\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        # Extract metrics\n",
        "        steps = [item[\"step\"] for item in self.evaluation_metrics]\n",
        "        rouge1 = [item.get(\"eval_rouge1\", 0) for item in self.evaluation_metrics]\n",
        "        rouge2 = [item.get(\"eval_rouge2\", 0) for item in self.evaluation_metrics]\n",
        "        rougeL = [item.get(\"eval_rougeL\", 0) for item in self.evaluation_metrics]\n",
        "        bleu = [item.get(\"eval_bleu\", 0) for item in self.evaluation_metrics]\n",
        "\n",
        "        # Plot metrics\n",
        "        plt.plot(steps, rouge1, label=\"ROUGE-1\")\n",
        "        plt.plot(steps, rouge2, label=\"ROUGE-2\")\n",
        "        plt.plot(steps, rougeL, label=\"ROUGE-L\")\n",
        "        plt.plot(steps, bleu, label=\"BLEU\")\n",
        "\n",
        "        plt.title(\"Evaluation Metrics\")\n",
        "        plt.xlabel(\"Step\")\n",
        "        plt.ylabel(\"Score\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig(os.path.join(self.log_dir, \"eval_metrics.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    def _plot_memory_usage(self):\n",
        "        \"\"\"Plot memory usage during training.\"\"\"\n",
        "        if not self.gpu_memory_stats:\n",
        "            return\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        # Extract memory stats\n",
        "        steps = [item[\"step\"] for item in self.gpu_memory_stats]\n",
        "        allocated = [item.get(\"gpu_allocated_gb\", 0) for item in self.gpu_memory_stats]\n",
        "        reserved = [item.get(\"gpu_reserved_gb\", 0) for item in self.gpu_memory_stats]\n",
        "\n",
        "        # Plot memory usage\n",
        "        plt.plot(steps, allocated, label=\"Allocated\")\n",
        "        plt.plot(steps, reserved, label=\"Reserved\")\n",
        "\n",
        "        plt.title(\"GPU Memory Usage\")\n",
        "        plt.xlabel(\"Step\")\n",
        "        plt.ylabel(\"Memory (GB)\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig(os.path.join(self.log_dir, \"memory_usage.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    def _save_final_report(self, state, final_metrics):\n",
        "        \"\"\"Create and save a comprehensive final report.\"\"\"\n",
        "        report = {\n",
        "            \"training_time_seconds\": self.training_time,\n",
        "            \"training_time_formatted\": str(datetime.timedelta(seconds=self.training_time)),\n",
        "            \"total_steps\": state.global_step,\n",
        "            \"average_step_time\": sum(self.step_times) / len(self.step_times) if self.step_times else 0,\n",
        "            \"peak_memory_usage\": max([s.get(\"gpu_allocated_gb\", 0) for s in self.gpu_memory_stats]) if self.gpu_memory_stats else 0,\n",
        "            \"final_metrics\": final_metrics,\n",
        "            \"best_metrics\": self.best_metrics\n",
        "        }\n",
        "\n",
        "        # Save to file\n",
        "        with open(os.path.join(self.log_dir, \"final_report.json\"), \"w\") as f:\n",
        "            json.dump(report, f, indent=2)\n",
        "\n",
        "        # Print report\n",
        "        logger.info(\"\\n\" + \"=\"*50)\n",
        "        logger.info(\"TRAINING COMPLETE - FINAL REPORT\")\n",
        "        logger.info(\"=\"*50)\n",
        "        logger.info(f\"Total training time: {report['training_time_formatted']}\")\n",
        "        logger.info(f\"Total steps: {report['total_steps']}\")\n",
        "        logger.info(f\"Average step time: {report['average_step_time']:.3f} seconds\")\n",
        "        logger.info(f\"Peak memory usage: {report['peak_memory_usage']:.2f} GB\")\n",
        "        logger.info(\"\\nBest metrics:\")\n",
        "        logger.info(f\"  ROUGE-1: {report['best_metrics']['rouge1']:.4f}\")\n",
        "        logger.info(f\"  ROUGE-2: {report['best_metrics']['rouge2']:.4f}\")\n",
        "        logger.info(f\"  ROUGE-L: {report['best_metrics']['rougeL']:.4f}\")\n",
        "        logger.info(f\"  BLEU: {report['best_metrics']['bleu']:.4f}\")\n",
        "        logger.info(f\"  (achieved at step {report['best_metrics']['step']})\")\n",
        "        logger.info(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1ufSp1nXwPNL"
      },
      "outputs": [],
      "source": [
        "def compute_metrics_function(eval_pred, tokenizer):\n",
        "    \"\"\"\n",
        "    Compute ROUGE and BLEU metrics for model evaluation.\n",
        "    \"\"\"\n",
        "    metrics_calculator = COMPUTEMETRICS()\n",
        "\n",
        "    # Unpack predictions and labels\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # For models that return loss and logits as a tuple\n",
        "    if isinstance(predictions, tuple):\n",
        "        predictions = predictions[0]\n",
        "\n",
        "    # If predictions are in logits format (3D tensor), convert to token IDs\n",
        "    if len(predictions.shape) == 3:\n",
        "        predictions = np.argmax(predictions, axis=-1)\n",
        "\n",
        "    # Create masks for valid label positions (where labels != -100)\n",
        "    label_mask = labels != -100\n",
        "\n",
        "    # Create decoded lists\n",
        "    decoded_preds = []\n",
        "    decoded_labels = []\n",
        "\n",
        "    # Process each sample individually to handle variable length properly\n",
        "    for pred, label, mask in zip(predictions, labels, label_mask):\n",
        "        # Filter out the padding and ignored positions\n",
        "        filtered_pred = pred[mask]\n",
        "        filtered_label = label[mask]\n",
        "\n",
        "        # Decode to text\n",
        "        pred_text = tokenizer.decode(filtered_pred, skip_special_tokens=True)\n",
        "        label_text = tokenizer.decode(filtered_label, skip_special_tokens=True)\n",
        "\n",
        "        decoded_preds.append(pred_text)\n",
        "        decoded_labels.append(label_text)\n",
        "\n",
        "    # Compute metrics\n",
        "    metrics = metrics_calculator.calculate_metrics(decoded_preds, decoded_labels)\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "cz8ZbuMmwXXm",
        "outputId": "38edf068-8a87-4e66-98f1-1149f1aa4f96"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdinesh-te\u001b[0m (\u001b[33mdinesh-te-northeastern-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250416_151029-k8davp6j</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dinesh-te-northeastern-university/ether-llama32/runs/k8davp6j' target=\"_blank\">polar-water-35</a></strong> to <a href='https://wandb.ai/dinesh-te-northeastern-university/ether-llama32' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/dinesh-te-northeastern-university/ether-llama32' target=\"_blank\">https://wandb.ai/dinesh-te-northeastern-university/ether-llama32</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/dinesh-te-northeastern-university/ether-llama32/runs/k8davp6j' target=\"_blank\">https://wandb.ai/dinesh-te-northeastern-university/ether-llama32/runs/k8davp6j</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "\n",
        "output_dir = \"./ether_llama32\"\n",
        "metrics_dir = os.path.join(output_dir, \"metrics\")\n",
        "model_name = \"unsloth/llama-3.2-3b-instruct\"\n",
        "learning_rate = 2e-3\n",
        "batch_size = 4\n",
        "gradient_accumulation_steps = 8\n",
        "use_ether_plus = True\n",
        "n_blocks = 16\n",
        "sample_size = 1000\n",
        "use_wandb = True\n",
        "\n",
        "# Setting random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Create output directories\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "os.makedirs(metrics_dir, exist_ok=True)\n",
        "\n",
        "# Initialize wandb if enabled\n",
        "if use_wandb:\n",
        "    wandb.init(\n",
        "        project=\"ether-llama32\",\n",
        "        config={\n",
        "            \"model_name\": model_name,\n",
        "            \"learning_rate\": learning_rate,\n",
        "            \"batch_size\": batch_size * gradient_accumulation_steps,\n",
        "            \"use_ether_plus\": use_ether_plus,\n",
        "            \"n_blocks\": n_blocks,\n",
        "            \"sample_size\": sample_size,\n",
        "            \"ether_variant\": \"ETHER+\" if use_ether_plus else \"ETHER\",\n",
        "        }\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Vdp7_ri4xH_5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Log initial memory\n",
        "logger.info(\"Initial GPU memory:\")\n",
        "initial_memory = get_memory_stats()\n",
        "logger.info(json.dumps(initial_memory, indent=2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBwgoEZExMfo",
        "outputId": "51cacac5-a442-41ce-df51-24a857a5392f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded in 5.20 seconds\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load model and tokenizer\n",
        "\n",
        "start_loading = time.time()\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "loading_time = time.time() - start_loading\n",
        "print(f\"Model loaded in {loading_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "aLACw91VxQ8B"
      },
      "outputs": [],
      "source": [
        "model.gradient_checkpointing_enable()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOGM1bo8xS6c",
        "outputId": "cc90b8ce-b2bf-4ad9-c627-adfa877f0b53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"gpu_allocated_gb\": 5.984213352203369,\n",
            "  \"gpu_reserved_gb\": 6.720703125,\n",
            "  \"gpu_max_allocated_gb\": 6.71875,\n",
            "  \"gpu_max_reserved_gb\": 6.720703125,\n",
            "  \"cpu_used_gb\": 3.4578285217285156,\n",
            "  \"cpu_total_gb\": 83.47704696655273,\n",
            "  \"cpu_percent\": 4.1\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Log memory after loading\n",
        "logger.info(\"GPU memory after model loading:\")\n",
        "post_loading_memory = get_memory_stats()\n",
        "print(json.dumps(post_loading_memory, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rA5yTQpmxWi4",
        "outputId": "46f305a7-f671-43b0-b651-5d1d868afc26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ETHER transformations applied in 0.20 seconds\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Applying ETHER transformations\n",
        "logger.info(\"Applying ETHER transformations...\")\n",
        "start_ether = time.time()\n",
        "\n",
        "model = apply_ether_to_model(\n",
        "    model,\n",
        "    use_ether_plus=True,\n",
        "    n_blocks=n_blocks,\n",
        "    double_sided=True\n",
        ")\n",
        "\n",
        "ether_time = time.time() - start_ether\n",
        "print(f\"ETHER transformations applied in {ether_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "f0ddf490d52a4685b7ec3aa192b59d6d",
            "7b3c5ddff7e64b32945a329e56e69586",
            "4f47437b13c14845b49ce8877a87fd99",
            "38d0e2f283e848399644dcdb797633aa",
            "341090f55c684f99bcc9a7b4cdd7687b",
            "fb88242df4aa4bac82eba5c5c7b25f97",
            "c25cb1421f02428cbc9a1fa87b45198e",
            "026d2535ff5f40428607c5c4c44f5e4f",
            "fc0f13895661429a90440e0a9f44bc36",
            "94b44072b9bd47bfb09f24c0eb1535e5",
            "3ac463580252422d8fa0e586c30c219f"
          ]
        },
        "id": "kxoPqM8mxZkd",
        "outputId": "5fa57a66-a60a-4fb9-ed92-87b0ee6a6495"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0ddf490d52a4685b7ec3aa192b59d6d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset prepared in 3.44 seconds\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Get dataset\n",
        "logger.info(\"Preparing dataset...\")\n",
        "start_dataset = time.time()\n",
        "\n",
        "datasets = load_alpaca_dataset(\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=512,\n",
        "    sample_size=sample_size\n",
        ")\n",
        "\n",
        "dataset_time = time.time() - start_dataset\n",
        "print(f\"Dataset prepared in {dataset_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwbQO_84xf8M",
        "outputId": "645f7aee-5bb0-436b-94cb-a98f1f477f0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on 900 examples, evaluating on 150 examples\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "train_dataset = datasets[\"train\"]\n",
        "eval_dataset = datasets[\"eval\"]\n",
        "print(f\"Training on {len(train_dataset)} examples, evaluating on {len(eval_dataset)} examples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_uTSYnkFxidY"
      },
      "outputs": [],
      "source": [
        "#  data collator\n",
        "data_collator = DATACOLLATOR(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Y0Pb_PyOxhqk"
      },
      "outputs": [],
      "source": [
        "# Create metrics tracker\n",
        "metrics_tracker = MetricsCallback(tokenizer, eval_dataset, log_dir=metrics_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "bJiPGlZfxxvR"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    return compute_metrics_function(eval_pred, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Ni2eX3pNx1ut"
      },
      "outputs": [],
      "source": [
        "#Setting training arguments\n",
        "training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=1,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=1,\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        learning_rate=learning_rate,\n",
        "\n",
        "        weight_decay=0.0,\n",
        "        logging_dir=os.path.join(output_dir, \"logs\"),\n",
        "        logging_steps=10,\n",
        "        save_steps=50,\n",
        "        eval_steps=10,\n",
        "        save_total_limit=2,\n",
        "\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        fp16=not torch.cuda.is_bf16_supported() and torch.cuda.is_available(),\n",
        "\n",
        "        dataloader_drop_last=False,\n",
        "        report_to=\"wandb\",\n",
        "        optim=\"adamw_torch\",\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        warmup_ratio=0.03,\n",
        "\n",
        "        remove_unused_columns=False,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Li2tMU26x-kG",
        "outputId": "37dcdf84-614f-4cf4-9e76-972837523f42"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-26-b23162370285>:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[metrics_tracker]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "tC57TXpayDSw"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Log memory before training\n",
        "logger.info(\"GPU memory before training:\")\n",
        "pre_training_memory = get_memory_stats()\n",
        "logger.info(json.dumps(pre_training_memory, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "zwuei3SlyF3E",
        "outputId": "85144e8c-3134-4a37-f901-daf9841813d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [28/28 23:51, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>10.938600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>7.997800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Train the model\n",
        "logger.info(\"Starting training...\")\n",
        "start_training = time.time()\n",
        "\n",
        "train_result = trainer.train()\n",
        "\n",
        "\n",
        "training_time = time.time() - start_training\n",
        "print(f\"Training completed in {datetime.timedelta(seconds=training_time)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2311ew7_yJm-",
        "outputId": "05b9ee72-6356-4e70-d82e-2b3b04b6010e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training metrics: {'train_runtime': 1483.8208255767822, 'train_samples_per_second': 0.607, 'train_steps_per_second': 0.019, 'train_loss': 8.908872604370117}\n"
          ]
        }
      ],
      "source": [
        "# Log training metrics\n",
        "train_metrics = {\n",
        "        \"train_runtime\": training_time,\n",
        "        \"train_samples_per_second\": train_result.metrics.get(\"train_samples_per_second\", 0),\n",
        "        \"train_steps_per_second\": train_result.metrics.get(\"train_steps_per_second\", 0),\n",
        "        \"train_loss\": train_result.metrics.get(\"train_loss\", 0),\n",
        "    }\n",
        "print(f\"Training metrics: {train_metrics}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "PdQB-Vv3XpYO"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "torch.cuda.synchronize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-CMiWHNxY3z",
        "outputId": "0fa3b64a-fedb-4b6e-b208-53e544b827ca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goMEZNhbyUXo",
        "outputId": "8ab30198-90f9-47b9-e83b-1af6d027c2c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"gpu_allocated_gb\": 15.362429141998291,\n",
            "  \"gpu_reserved_gb\": 36.0625,\n",
            "  \"gpu_max_allocated_gb\": 15.362429141998291,\n",
            "  \"gpu_max_reserved_gb\": 36.0625,\n",
            "  \"cpu_used_gb\": 5.143348693847656,\n",
            "  \"cpu_total_gb\": 83.47704696655273,\n",
            "  \"cpu_percent\": 6.2\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Print final memory stats\n",
        "logger.info(\"Final GPU memory:\")\n",
        "final_memory = get_memory_stats()\n",
        "print(json.dumps(final_memory, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "4bV9ELE8yc-l"
      },
      "outputs": [],
      "source": [
        "\n",
        "summary = {\n",
        "    \"model\": model_name,\n",
        "    \"ether_variant\": \"ETHER+\" if use_ether_plus else \"ETHER\",\n",
        "    \"blocks\": n_blocks,\n",
        "    \"dataset_size\": len(train_dataset),\n",
        "    \"times\": {\n",
        "        \"loading\": loading_time,\n",
        "        \"ether_application\": ether_time,\n",
        "        \"dataset_preparation\": dataset_time,\n",
        "        \"training\": training_time,\n",
        "        \"evaluation\": eval_time,\n",
        "        \"total\": loading_time + ether_time + dataset_time + training_time + eval_time\n",
        "    },\n",
        "    \"memory\": {\n",
        "        \"initial\": initial_memory,\n",
        "        \"after_loading\": post_loading_memory,\n",
        "        \"before_training\": pre_training_memory,\n",
        "        \"final\": final_memory,\n",
        "        \"peak_allocated\": max([s.get(\"gpu_allocated_gb\", 0) for s in metrics_tracker.gpu_memory_stats]) if metrics_tracker.gpu_memory_stats else 0\n",
        "    },\n",
        "    \"train_metrics\": train_metrics\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sI327YIhw8S5",
        "outputId": "2f5c4e5d-2bed-415f-8af5-f208464f21fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "ETHER TRAINING SUMMARY\n",
            "==================================================\n",
            "Model: unsloth/llama-3.2-3b-instruct\n",
            "Variant: ETHER+ with 16 blocks\n",
            "Dataset size: 900 examples\n",
            "==================================================\n",
            "\n",
            "Time statistics:\n",
            "  Model loading: 5.20 seconds\n",
            "  ETHER application: 0.20 seconds\n",
            "  Dataset preparation: 3.44 seconds\n",
            "  Training: 0:24:43.820826\n",
            "  Evaluation: 72.02 seconds\n",
            "  Total: 0:26:04.679760\n",
            "==================================================\n",
            "\n",
            "Memory statistics:\n",
            "  Initial: 0.00 GB\n",
            "  After loading: 5.98 GB\n",
            "  Peak during training: 15.36 GB\n",
            "  Final: 15.36 GB\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Print final summary\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ETHER TRAINING SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Model: {summary['model']}\")\n",
        "print(f\"Variant: {summary['ether_variant']} with {summary['blocks']} blocks\")\n",
        "print(f\"Dataset size: {summary['dataset_size']} examples\")\n",
        "print(\"=\"*50)\n",
        "print(\"\\nTime statistics:\")\n",
        "print(f\"  Model loading: {summary['times']['loading']:.2f} seconds\")\n",
        "print(f\"  ETHER application: {summary['times']['ether_application']:.2f} seconds\")\n",
        "print(f\"  Dataset preparation: {summary['times']['dataset_preparation']:.2f} seconds\")\n",
        "print(f\"  Training: {datetime.timedelta(seconds=summary['times']['training'])}\")\n",
        "print(f\"  Evaluation: {summary['times']['evaluation']:.2f} seconds\")\n",
        "print(f\"  Total: {datetime.timedelta(seconds=summary['times']['total'])}\")\n",
        "print(\"=\"*50)\n",
        "print(\"\\nMemory statistics:\")\n",
        "print(f\"  Initial: {summary['memory']['initial']['gpu_allocated_gb']:.2f} GB\")\n",
        "print(f\"  After loading: {summary['memory']['after_loading']['gpu_allocated_gb']:.2f} GB\")\n",
        "print(f\"  Peak during training: {summary['memory']['peak_allocated']:.2f} GB\")\n",
        "print(f\"  Final: {summary['memory']['final']['gpu_allocated_gb']:.2f} GB\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXJOv-n_yROl",
        "outputId": "61a64536-dc7c-4c48-b033-24b4d73e095d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./ether-lora-optimized/tokenizer_config.json',\n",
              " './ether-lora-optimized/special_tokens_map.json',\n",
              " './ether-lora-optimized/tokenizer.json')"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.save_model(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJh9fJtl_Byu"
      },
      "outputs": [],
      "source": [
        "def evaluate_base_model(model,eval_dataset, tokenizer, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the evaluation dataset\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    metrics_calculator = COMPUTEMETRICS()\n",
        "\n",
        "    all_decoded_preds = []\n",
        "    all_decoded_labels = []\n",
        "    examples_to_show = 5\n",
        "\n",
        "\n",
        "    eval_dataloader = DataLoader(\n",
        "        eval_dataset,\n",
        "        batch_size=2,  \n",
        "        collate_fn=data_collator\n",
        "    )\n",
        "\n",
        "    print(f\"Processing {len(eval_dataset)} evaluation examples\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(eval_dataloader):\n",
        "            # Move batch to device\n",
        "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
        "\n",
        "            # Get model output\n",
        "            outputs = model(**batch)\n",
        "\n",
        "            # Get predictions\n",
        "            logits = outputs.logits\n",
        "            predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "            labels = batch[\"labels\"].cpu().numpy()\n",
        "\n",
        "            # Process each example in the batch individually\n",
        "            for i in range(predictions.shape[0]):\n",
        "                pred = predictions[i]\n",
        "                label = labels[i]\n",
        "\n",
        "                # Replace -100 with padding token\n",
        "                label_mask = label != -100\n",
        "                filtered_label = label[label_mask]\n",
        "\n",
        "                # Find where the actual output starts (where labels are not -100)\n",
        "                valid_positions = np.where(label_mask)[0]\n",
        "                if len(valid_positions) > 0:\n",
        "                    # Get the prediction outputs only for valid positions\n",
        "                    filtered_pred = pred[valid_positions]\n",
        "\n",
        "                    # Decode text\n",
        "                    try:\n",
        "                        pred_text = tokenizer.decode(filtered_pred, skip_special_tokens=True)\n",
        "                        label_text = tokenizer.decode(filtered_label, skip_special_tokens=True)\n",
        "\n",
        "                        all_decoded_preds.append(pred_text)\n",
        "                        all_decoded_labels.append(label_text)\n",
        "\n",
        "                        # Print some examples to inspect\n",
        "                        example_idx = batch_idx * batch[\"input_ids\"].shape[0] + i\n",
        "                        if example_idx < examples_to_show:\n",
        "                            print(f\"\\nExample {example_idx+1}:\")\n",
        "                            # Get the input prompt\n",
        "                            input_ids = batch[\"input_ids\"][i].cpu().numpy()\n",
        "                            prompt = tokenizer.decode(input_ids, skip_special_tokens=False)\n",
        "                            # Find where the assistant's response starts\n",
        "                            assistant_start = prompt.rfind(\"<|start_header_id|>assistant<|end_header_id|>\")\n",
        "                            if assistant_start != -1:\n",
        "                                prompt = prompt[:assistant_start + len(\"<|start_header_id|>assistant<|end_header_id|>\")]\n",
        "                            print(f\"Input prompt: {prompt}\")\n",
        "                            print(f\"Prediction: {pred_text}\")\n",
        "                            print(f\"Reference: {label_text}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error decoding example: {e}\")\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = metrics_calculator.calculate_metrics(all_decoded_preds, all_decoded_labels)\n",
        "\n",
        "    # Print overall metrics\n",
        "    print(\"\\nEvaluation Metrics for Base Model:\")\n",
        "    print(f\"ROUGE-1: {metrics['rouge1']:.4f}\")\n",
        "    print(f\"ROUGE-2: {metrics['rouge2']:.4f}\")\n",
        "    print(f\"ROUGE-L: {metrics['rougeL']:.4f}\")\n",
        "    print(f\"BLEU: {metrics['bleu']:.4f}\")\n",
        "\n",
        "    # Print comparison for a few examples\n",
        "    print(\"\\n====== Detailed Comparison of Base Model vs Expected Output ======\")\n",
        "    for i in range(min(examples_to_show, len(all_decoded_preds))):\n",
        "        pred = all_decoded_preds[i]\n",
        "        ref = all_decoded_labels[i]\n",
        "\n",
        "        # Calculate individual metrics for this example\n",
        "        rouge_scores = metrics_calculator.calculate_rouge(pred, ref)\n",
        "        bleu_score = metrics_calculator.calculate_bleu(pred, ref)\n",
        "\n",
        "        print(f\"\\n--- Example {i+1} ---\")\n",
        "        print(f\"Base Model Output: {pred[:200]}...\" if len(pred) > 200 else f\"Base Model Output: {pred}\")\n",
        "        print(f\"Expected Output: {ref[:200]}...\" if len(ref) > 200 else f\"Expected Output: {ref}\")\n",
        "        print(f\"Individual Metrics:\")\n",
        "        print(f\"  ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
        "        print(f\"  ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
        "        print(f\"  ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
        "        print(f\"  BLEU: {bleu_score:.4f}\")\n",
        "\n",
        "    # Clean up to free memory\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return {\n",
        "        \"predictions\": all_decoded_preds,\n",
        "        \"references\": all_decoded_labels,\n",
        "        \"metrics\": metrics\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gp9tA_g8SRGO",
        "outputId": "fdff3556-e375-43c4-d682-9b205831a6e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 150 evaluation examples\n",
            "\n",
            "Example 1:\n",
            "Input prompt: <|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.<|start_header_id|>user<|end_header_id|>\n",
            "Calculate the mass of 4.5 moles of carbon dioxide.<|start_header_id|>assistant<|end_header_id|>\n",
            "Prediction: The mass of 4.5 moles of carbon dioxide is 4 grams0 grams. is be calculated by multiplying the4.5 moles by the molar mass of carbon dioxide, which is 44.01 grams per mole.\n",
            "Reference: The mass of 4.5 moles of carbon dioxide is 324.75 grams. This can be calculated by multiplying 4.5 moles by the molar mass of carbon dioxide, which is 44.01 grams per mole.\n",
            "\n",
            "Example 2:\n",
            "Input prompt: <|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.<|start_header_id|>user<|end_header_id|>\n",
            "Rewrite the following text as a third-person narrative.\n",
            "I am excited to go on a vacation.<|start_header_id|>assistant<|end_header_id|>\n",
            "Prediction: The is excited to go on a vacation. She\n",
            "Reference: She is excited to go on a vacation.\n",
            "\n",
            "Example 3:\n",
            "Input prompt: <|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.<|start_header_id|>user<|end_header_id|>\n",
            "You need to summarize a text in three sentences.\n",
            "The Santa Ana winds are strong, hot, dry winds that originate inland and affect coastal Southern California and northern Baja California, particularly in the late summer and fall. These winds, which usually arrive from October to March, can cause wildfires in the region, and can reach speeds of up to 80 miles per hour. These winds are created when high pressure air from the Great Basin of Nevada and Arizona moves towards the coast and decrease in elevation as it approaches the Pacific Ocean.<|start_header_id|>assistant<|end_header_id|>\n",
            "Prediction: The Santa Ana winds are strong, hot, dry dry winds that affect inland the Great Basin of affect to the coast Ocean, They winds can cause wildfires and Southern region and can speeds of up to 80 miles per hour. The are from October to March and\n",
            "Reference: The Santa Ana winds are strong, hot, and dry winds that originate in the Great Basin and travel towards the Pacific Ocean. These winds can cause wildfires in the region and reach speeds of up to 80 miles an hour. They occur from October to March.\n",
            "\n",
            "Example 4:\n",
            "Input prompt: <|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.<|start_header_id|>user<|end_header_id|>\n",
            "Generate 3 ideas for modifying an existing widget.<|start_header_id|>assistant<|end_header_id|>\n",
            "Prediction: The. Add a extra computer assistant system to the widget. that can control it device with voice voice.2. Addegrate the augmented-powered that the object and and predictive analysis.\n",
            "3. Add a charging capabilities to the widget to it can be charged wire the cables power.\n",
            "Reference: 1. Add an onboard voice command system to the widget so users can operate the device with their voice.\n",
            "2. Integrate an AI system for automatic performance optimization and data analysis.\n",
            "3. Embed wireless charging technology in the widget so it can be charged without any additional cable.\n",
            "\n",
            "Example 5:\n",
            "Input prompt: <|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.<|start_header_id|>user<|end_header_id|>\n",
            "Infer the meaning of the following idiom: \"It's raining cats and dogs.\"<|start_header_id|>assistant<|end_header_id|>\n",
            "Prediction: The idiom means used to describe a heavy down. It is that it rain is so heavy that heavy that it's like if cats and dogs are falling falling from the sky.\n",
            "Reference: This idiom is used to describe a heavy rainfall. It suggests that the rainfall is so hard and fast that it is as if cats and dogs are literally falling from the sky.\n",
            "\n",
            "Evaluation Metrics for Base Model:\n",
            "ROUGE-1: 0.6278\n",
            "ROUGE-2: 0.3929\n",
            "ROUGE-L: 0.5824\n",
            "BLEU: 0.3305\n",
            "\n",
            "====== Detailed Comparison of Base Model vs Expected Output ======\n",
            "\n",
            "--- Example 1 ---\n",
            "Base Model Output: The mass of 4.5 moles of carbon dioxide is 4 grams0 grams. is be calculated by multiplying the4.5 moles by the molar mass of carbon dioxide, which is 44.01 grams per mole.\n",
            "Expected Output: The mass of 4.5 moles of carbon dioxide is 324.75 grams. This can be calculated by multiplying 4.5 moles by the molar mass of carbon dioxide, which is 44.01 grams per mole.\n",
            "Individual Metrics:\n",
            "  ROUGE-1: 0.9014\n",
            "  ROUGE-2: 0.7826\n",
            "  ROUGE-L: 0.8732\n",
            "  BLEU: 0.7440\n",
            "\n",
            "--- Example 2 ---\n",
            "Base Model Output: The is excited to go on a vacation. She\n",
            "Expected Output: She is excited to go on a vacation.\n",
            "Individual Metrics:\n",
            "  ROUGE-1: 0.9412\n",
            "  ROUGE-2: 0.8000\n",
            "  ROUGE-L: 0.8235\n",
            "  BLEU: 0.7825\n",
            "\n",
            "--- Example 3 ---\n",
            "Base Model Output: The Santa Ana winds are strong, hot, dry dry winds that affect inland the Great Basin of affect to the coast Ocean, They winds can cause wildfires and Southern region and can speeds of up to 80 miles ...\n",
            "Expected Output: The Santa Ana winds are strong, hot, and dry winds that originate in the Great Basin and travel towards the Pacific Ocean. These winds can cause wildfires in the region and reach speeds of up to 80 mi...\n",
            "Individual Metrics:\n",
            "  ROUGE-1: 0.7742\n",
            "  ROUGE-2: 0.4835\n",
            "  ROUGE-L: 0.6882\n",
            "  BLEU: 0.4176\n",
            "\n",
            "--- Example 4 ---\n",
            "Base Model Output: The. Add a extra computer assistant system to the widget. that can control it device with voice voice.2. Addegrate the augmented-powered that the object and and predictive analysis.\n",
            "3. Add a charging ...\n",
            "Expected Output: 1. Add an onboard voice command system to the widget so users can operate the device with their voice.\n",
            "2. Integrate an AI system for automatic performance optimization and data analysis.\n",
            "3. Embed wire...\n",
            "Individual Metrics:\n",
            "  ROUGE-1: 0.4842\n",
            "  ROUGE-2: 0.2151\n",
            "  ROUGE-L: 0.4421\n",
            "  BLEU: 0.1400\n",
            "\n",
            "--- Example 5 ---\n",
            "Base Model Output: The idiom means used to describe a heavy down. It is that it rain is so heavy that heavy that it's like if cats and dogs are falling falling from the sky.\n",
            "Expected Output: This idiom is used to describe a heavy rainfall. It suggests that the rainfall is so hard and fast that it is as if cats and dogs are literally falling from the sky.\n",
            "Individual Metrics:\n",
            "  ROUGE-1: 0.6970\n",
            "  ROUGE-2: 0.4375\n",
            "  ROUGE-L: 0.6364\n",
            "  BLEU: 0.3621\n"
          ]
        }
      ],
      "source": [
        "model_results = evaluate_base_model(model,eval_dataset, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BK5R55hR9i1",
        "outputId": "3aba1683-8e93-4e29-abf2-22ec8374d1bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'rouge1': 0.627784442891029, 'rouge2': 0.3929223310724818, 'rougeL': 0.5823602828416555, 'bleu': 0.3304549498825494}\n"
          ]
        }
      ],
      "source": [
        "print(model_results[\"metrics\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpQZBBi9TwrQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "026d2535ff5f40428607c5c4c44f5e4f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "341090f55c684f99bcc9a7b4cdd7687b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38d0e2f283e848399644dcdb797633aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94b44072b9bd47bfb09f24c0eb1535e5",
            "placeholder": "â",
            "style": "IPY_MODEL_3ac463580252422d8fa0e586c30c219f",
            "value": "â1000/1000â[00:00&lt;00:00,â1946.02âexamples/s]"
          }
        },
        "3ac463580252422d8fa0e586c30c219f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f47437b13c14845b49ce8877a87fd99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_026d2535ff5f40428607c5c4c44f5e4f",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fc0f13895661429a90440e0a9f44bc36",
            "value": 1000
          }
        },
        "7b3c5ddff7e64b32945a329e56e69586": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb88242df4aa4bac82eba5c5c7b25f97",
            "placeholder": "â",
            "style": "IPY_MODEL_c25cb1421f02428cbc9a1fa87b45198e",
            "value": "Map:â100%"
          }
        },
        "94b44072b9bd47bfb09f24c0eb1535e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c25cb1421f02428cbc9a1fa87b45198e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0ddf490d52a4685b7ec3aa192b59d6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7b3c5ddff7e64b32945a329e56e69586",
              "IPY_MODEL_4f47437b13c14845b49ce8877a87fd99",
              "IPY_MODEL_38d0e2f283e848399644dcdb797633aa"
            ],
            "layout": "IPY_MODEL_341090f55c684f99bcc9a7b4cdd7687b"
          }
        },
        "fb88242df4aa4bac82eba5c5c7b25f97": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc0f13895661429a90440e0a9f44bc36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
